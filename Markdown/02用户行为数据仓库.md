# 一、数仓分层

## 1.概念

### 1.1 分层原因

>**复杂问题简单化**
将一个复杂的**任务分解**成多个步骤，每层只处理单一的步骤，**比较简单，方便定位问题**

>**减少重复开发**
规范数据分层，通过的**中间层数据**，能够减少极大的重复计算，增加一次计算结果的复用性

>**隔离原始数据**
不论是数据的异常还是数据的敏感性，使真是数据与统计数据**解耦**开

### 1.2 数仓分层

>**ODS层(原始数据层)**
存放原始数据，直接加载原始日志、数据，数据保持原貌不作处理

>**DWD层(明细数据层)**
结构和粒度与原始表保持一致，**ODS层数据进行清洗(去除空值、脏数据、超过极限范围的数据)**，个别公司称为DWI

>**DWS层(服务数据层)**
以DWD为基础，**进行数据轻度汇总**，一般聚集到以用户当日、设备当日、商家当日、商品当日等等的粒度
通常会有以某个维度为线索，组成跨主题的宽表，比如，一个用户的当日的签到数、收藏数、评论数、抽奖数、订阅数、点赞数、流浪商品数、添加购物车数、下单数、支持数、退款数7点击广告数组成的列表

>**ADS层(数据应用层)**
**为各种统计报表提供数据**，也有称为APP层、DM等

## 2.数据集市和数据仓库

**数据集市(Data Market)**是一种**微型的数据仓库**，通常有更少的数据，更少的主题区域，以及更少的历史数据，因此是**部门级**的，一般只能为某个局部范围内的管理人员服务
**数据仓库**是**企业级**的，能为整个企业各个部门的运行提供决策支持手段
 * 数据集市现在市面上的公司和书籍对数据集市有不同的概念

## 3.数仓命名规范

 * ODS层 --- ods
 * DWD层 --- dwd
 * DWS层 --- dws
 * ADS层 --- ads
 * 临时表数据库命名为 xxx-tmp
 * 备份数据数据库命名为 xxx_bak


# 二、数仓搭建环境准备

**集群规划**
|       | hadoop102 | hadoop103 | hadoop104 |
| :---- | :-------- | :-------- | :-------- |
| Hive  | Hive      |           |           |
| MySQL | MySQL     |           |           |

## 1.Hive & MySQL

[**安装配置步骤**](link/steps.md)

```bash
# 关闭元数据检查
vim /opt/module/hive/conf/hive-site.xml
```
```xml
<property>
    <name>hive.metastore.schema.verification</name>
    <value>false</value>
</property>
```

## 2.Hive引擎Tez
Tez是一个Hive的运行引擎，性能优于MR。为什么优于MR呢？看下图。
<!-- TODO Tez配图 -->
![]()
用Hive直接编写MR程序，假设有四个有依赖关系的MR作业，上图中，绿色是Reduce Task，云状表示写屏蔽，需要将中间结果持久化写到HDFS。
Tez可以将多个有依赖的作业转换为一个作业，这样只需写一次HDFS，且中间节点较少，从而大大提升作业的计算性能。

### 2.1 安装配置

[**Tez安装配置上传测试**](link/steps.md)

### 2.2 问题总结

>**运行Tez检查到用过多内存而被NodeManager杀死进程**
Caused by: org.apache.tez.dag.api.SessionNotRunning: TezSession has already shutdown. Application application_1546781144082_0005 failed 2 times due to AM Container for appattempt_1546781144082_0005_000002 exited with  exitCode: -103
For more detailed output, check application tracking page:http://hadoop103:8088/cluster/app/application_1546781144082_0005Then, click on links to logs of each attempt.
Diagnostics: Container [pid=11116,containerID=container_1546781144082_0005_02_000001] is running beyond virtual memory limits. Current usage: 216.3 MB of 1 GB physical memory used; 2.6 GB of 2.1 GB virtual memory used. Killing container.

>**问题原因:从机上运行的Container试图使用过多的内存，而被NodeManager kill掉了**
The NodeManager is killing your container. It sounds like you are trying to use hadoop streaming which is running as a child process of the map-reduce task. The NodeManager monitors the entire process tree of the task and if it eats up more memory than the maximum set in mapreduce.map.memory.mb or mapreduce.reduce.memory.mb respectively, we would expect the Nodemanager to kill the task, otherwise your task is stealing memory belonging to other containers, which you don't want.

>**解决方法，修改yarn-site.xml重启集群**
```xml
<property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
</property>
```

## 3.元数据备份

元数据备份（重点，如数据损坏，可能整个集群无法运行，至少要保证每日零点之后备份到其它服务器两个复本）
<!-- TODO ... -->

# 三、ODS层搭建

